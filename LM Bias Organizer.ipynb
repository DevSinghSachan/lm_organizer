{
 "metadata": {
  "name": "",
  "signature": "sha256:608a4485d946dd25c82622d3e2e832577dca025d659dfa5e39582408f2163c32"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Language Model Organizer\n",
      "\n",
      "Language model built around the idea of **binary search**. Method is simple, take any input text, assign a tree to the text by recursively cutting it in halves. Take random words inside and train the vector to separate the words into the tree (hierarchical softmax in a minimalistic setting)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%config InlineBackend.figure_format = 'svg'\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt, os, numpy as np, time\n",
      "from model import OrganizerModel, BiasOrganizerModel\n",
      "from IPython.display import clear_output\n",
      "from model.utils import DocumentTree, code_to_index, number_of_branches_per_depth, codes_for_depth, codes_upto_depth\n",
      "from model.utils import import_mini_wiki_corpus, collect_counts, save_object, load_object"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "collect words and documents using hash table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = import_mini_wiki_corpus(\"/users/jonathanraiman/desktop/plaintext_articles/\")\n",
      "if os.path.exists(\"saves/vocab.gz\"):\n",
      "    vocab = load_object(\"saves/vocab.gz\")\n",
      "else:\n",
      "    vocab = collect_counts(corpus)\n",
      "    save_object(vocab, \"saves/vocab.gz\")\n",
      "\n",
      "# convert words:\n",
      "NUM_WORDS = 50000\n",
      "index2word = [word[0] for word in vocab.most_common(NUM_WORDS)]\n",
      "word2index = {}\n",
      "for i, word in enumerate(index2word):\n",
      "    word2index[word] = i\n",
      "   \n",
      "# convert documents:\n",
      "index2document = []\n",
      "document2index = {}\n",
      "if os.path.exists(\"saves/documents.gz\"):\n",
      "    documents = load_object(\"saves/documents.gz\")\n",
      "    index2document = []\n",
      "    document2index = {}\n",
      "    for document in documents:\n",
      "        document2index[document.name]= document.index\n",
      "        index2document.append(document.name)\n",
      "else:\n",
      "    document_index = 0\n",
      "    documents = []\n",
      "    for key, value in corpus.items():\n",
      "        index2document.append(key)\n",
      "        document2index[key] = document_index\n",
      "        documents.append(DocumentTree(key, value, word2index, document_index))\n",
      "        document_index+=1\n",
      "    save_object(documents, \"saves/documents.gz\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "build model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizer_model = OrganizerModel(size = 50,\n",
      "                                 tree_depth = 2,\n",
      "                                 learning_rate = 1.0,\n",
      "                                vocabulary_size = NUM_WORDS,\n",
      "                                document_size = len(documents))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizer_model = BiasOrganizerModel(size = 50,\n",
      "                                 tree_depth = 2,\n",
      "                                 learning_rate = 0.035,\n",
      "                                vocabulary_size = NUM_WORDS,\n",
      "                                document_size = len(documents))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 50\n",
      "epoch_cost = 0.0\n",
      "SAMPLE_SIZE = 10\n",
      "# label with 0 for top section\n",
      "# 1 for bottom section of code:\n",
      "SAMPLE_LABELINGS = np.concatenate([np.zeros(SAMPLE_SIZE, dtype=np.int32), np.ones(SAMPLE_SIZE, dtype=np.int32)])\n",
      "\n",
      "codes = [(code, code_to_index(organizer_model.tree_depth,\n",
      "                              code)) for code in codes_upto_depth(organizer_model.tree_depth)]\n",
      "DOCSET = documents[0:100]\n",
      "\n",
      "WINDOWS = 3\n",
      "\n",
      "TOTAL_DOCS = len(DOCSET) / 100.\n",
      "megaerrors = []\n",
      "\n",
      "def window_training_loop(model, doc, code, branch_offset):\n",
      "    cost = 0.0\n",
      "    for window in range(WINDOWS):\n",
      "        cost += model.update_fun(\n",
      "            doc.create_example(code, top = True, sample_size = SAMPLE_SIZE),\n",
      "            doc.index,\n",
      "            branch_offset,\n",
      "            [0])\n",
      "        cost += model.update_fun(\n",
      "            doc.create_example(code, top = False, sample_size = SAMPLE_SIZE),\n",
      "            doc.index,\n",
      "            branch_offset, [1])\n",
      "    return cost\n",
      "\n",
      "def sample_training_loop(model, doc, code, branch_offset):\n",
      "    sample = np.concatenate([\n",
      "        doc.create_example(code, top = True, sample_size = SAMPLE_SIZE),\n",
      "        doc.create_example(code, top = False, sample_size = SAMPLE_SIZE)\n",
      "        ])\n",
      "\n",
      "    # train on them:\n",
      "    return model.update_fun(sample, doc.index, branch_offset, SAMPLE_LABELINGS)\n",
      "\n",
      "training_loop = window_training_loop if issubclass(BiasOrganizerModel, type(organizer_model)) else sample_training_loop\n",
      "\n",
      "for epoch in range(epochs):\n",
      "    t1 = time.time()\n",
      "    epoch_cost = 0.0\n",
      "    for k, doc in enumerate(DOCSET):\n",
      "        # for each code (including the null code, parent node)\n",
      "        # we train the vectors using the same words:\n",
      "        for code, branch_offset in codes:\n",
      "            epoch_cost += training_loop(organizer_model, doc, code, branch_offset)\n",
      "        if k % 10 == 0:\n",
      "            clear_output(wait=True)\n",
      "            print(\"epoch: %d, progress: %.1f%%, %.2f docs/s, cost: %.2f\" % (epoch, k/TOTAL_DOCS, k / (time.time() - t1), epoch_cost))\n",
      "    megaerrors.append(epoch_cost)\n",
      "    organizer_model.reset_adagrad()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch: 49, progress: 90.0%, 4.92 docs/s, cost: 2650.26\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Inspect results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def most_similar_word(matrix, index2label, label2index, word, topn = 10):\n",
      "        index = label2index[word]\n",
      "        word = matrix[index]\n",
      "        dists = np.linalg.norm(matrix - word, axis=1)\n",
      "        best = np.argsort(dists)[:topn + 1]\n",
      "        result = [(index2label[sim], float(dists[sim]), sim) for sim in best if sim != index]\n",
      "        return result[:topn]\n",
      "def most_similar_doc(matrix, index2label, label2index, word, topn = 10, code = []):\n",
      "        branch_index = code_to_index(organizer_model.tree_depth, code)\n",
      "        index = label2index[word]\n",
      "        word = matrix[index]#, branch_index]\n",
      "        dists = np.dot(matrix, word).astype(np.float32)\n",
      "        best = np.argsort(dists)[::-1][:topn + 1]\n",
      "        result = [(index2label[sim], float(dists[sim]), sim) for sim in best if sim != index]\n",
      "        return result[:topn]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm_doc_matrix = organizer_model.document_matrix.get_value(borrow=True).reshape(-1, 50 * 7)[:, 0:organizer_model.size]\n",
      "norm_doc_matrix = (norm_doc_matrix / np.sqrt((norm_doc_matrix ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "norm_model_matrix = organizer_model.model_matrix.get_value(borrow=True)\n",
      "norm_model_matrix = (norm_model_matrix / np.sqrt((norm_model_matrix ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_similar_word(norm_model_matrix,\n",
      "                  index2word, \n",
      "                  word2index,\n",
      "                  \"rich\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "[('re-established', 0.9700818657875061, 14884),\n",
        " ('Magma', 0.9827530980110168, 36757),\n",
        " ('madman', 1.010695457458496, 39876),\n",
        " ('Venturers', 1.012843370437622, 39471),\n",
        " ('Got', 1.0129207372665405, 17703),\n",
        " ('intolerant', 1.0141044855117798, 47882),\n",
        " ('essays', 1.029833436012268, 7726),\n",
        " ('stencil', 1.0310086011886597, 38216),\n",
        " ('Sue', 1.0326958894729614, 19603),\n",
        " ('dark', 1.032885193824768, 1576)]"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_similar_doc(norm_doc_matrix,\n",
      "                  index2document,\n",
      "                  document2index,\n",
      "                  \"The_Adventures_of_Tintin.txt\",\n",
      "                  code = [])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "[('Rain.txt', 0.5126614570617676, 2257),\n",
        " ('Oligocene.txt', 0.45710593461990356, 942),\n",
        " ('Engineering.txt', 0.4561893343925476, 2049),\n",
        " ('Moscow.txt', 0.45431551337242126, 929),\n",
        " ('United_States_Numbered_Highways.txt', 0.44120165705680847, 3789),\n",
        " ('Andrew_Robinson.txt', 0.4377230107784271, 1747),\n",
        " ('The_Lorax.txt', 0.4356021285057068, 316),\n",
        " ('Donald_Bradman.txt', 0.41823020577430725, 636),\n",
        " ('Arthur_Wellesley%2C_1st_Duke_of_Wellington.txt', 0.41573601961135864, 2258),\n",
        " ('Multiple_sequence_alignment.txt', 0.4135739505290985, 3097)]"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}